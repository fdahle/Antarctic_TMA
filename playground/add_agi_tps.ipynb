{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:42:56.908456Z",
     "start_time": "2024-08-15T16:42:56.886813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "8f6a57d3bb739851",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:42:56.920836Z",
     "start_time": "2024-08-15T16:42:56.910238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_folder = \"/home/fdahle/Desktop/agi_test/images\"\n",
    "msk_folder = \"/home/fdahle/Desktop/agi_test/masks\""
   ],
   "id": "8d93631926370490",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:42:59.618279Z",
     "start_time": "2024-08-15T16:42:59.604933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import src.base.create_mask as cm\n",
    "import src.load.load_image as li\n",
    "import src.export.export_tiff as et\n",
    "\n",
    "import os\n",
    "for img in os.listdir(img_folder):\n",
    "    \n",
    "    img_path = os.path.join(img_folder, img)\n",
    "    img_id = img.split(\".\")[0]\n",
    "    print(img_path)\n",
    "    \n",
    "    # load image\n",
    "    img = li.load_image(img_path)\n",
    "    mask = cm.create_mask(img, use_database=True, image_id=img_id)\n",
    "    et.export_tiff(mask, os.path.join(msk_folder, f\"{img_id}_mask.tif\"), overwrite=True)\n",
    "\"\"\""
   ],
   "id": "dce4fc6db4fcb357",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport src.base.create_mask as cm\\nimport src.load.load_image as li\\nimport src.export.export_tiff as et\\n\\nimport os\\nfor img in os.listdir(img_folder):\\n    \\n    img_path = os.path.join(img_folder, img)\\n    img_id = img.split(\".\")[0]\\n    print(img_path)\\n    \\n    # load image\\n    img = li.load_image(img_path)\\n    mask = cm.create_mask(img, use_database=True, image_id=img_id)\\n    et.export_tiff(mask, os.path.join(msk_folder, f\"{img_id}_mask.tif\"), overwrite=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:43:00.161399Z",
     "start_time": "2024-08-15T16:43:00.151410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import src.base.rotate_image as ri\n",
    "import src.load.load_image as li\n",
    "import src.export.export_tiff as et\n",
    "import os\n",
    "\n",
    "# rotate vertical images \n",
    "for img in os.listdir(img_folder):\n",
    "    if \"V\" in img:\n",
    "        print(img)\n",
    "        img_path = os.path.join(img_folder, img)\n",
    "        img_id = img.split(\".\")[0]\n",
    "        img = li.load_image(img_path)\n",
    "        img = ri.rotate_image(img, 180)\n",
    "        et.export_tiff(img, img_path, overwrite=True)\n",
    "\n",
    "for img in os.listdir(msk_folder):\n",
    "    if \"V\" in img:\n",
    "        print(img)\n",
    "        img_path = os.path.join(msk_folder, img)\n",
    "        img_id = img.split(\".\")[0]\n",
    "        img = li.load_image(img_path)\n",
    "        img = ri.rotate_image(img, 180)\n",
    "        et.export_tiff(img, img_path, overwrite=True)\n",
    "\"\"\""
   ],
   "id": "ca125d76807653b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport src.base.rotate_image as ri\\nimport src.load.load_image as li\\nimport src.export.export_tiff as et\\nimport os\\n\\n# rotate vertical images \\nfor img in os.listdir(img_folder):\\n    if \"V\" in img:\\n        print(img)\\n        img_path = os.path.join(img_folder, img)\\n        img_id = img.split(\".\")[0]\\n        img = li.load_image(img_path)\\n        img = ri.rotate_image(img, 180)\\n        et.export_tiff(img, img_path, overwrite=True)\\n\\nfor img in os.listdir(msk_folder):\\n    if \"V\" in img:\\n        print(img)\\n        img_path = os.path.join(msk_folder, img)\\n        img_id = img.split(\".\")[0]\\n        img = li.load_image(img_path)\\n        img = ri.rotate_image(img, 180)\\n        et.export_tiff(img, img_path, overwrite=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:08:49.999709Z",
     "start_time": "2024-08-15T08:08:43.267605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import Metashape\n",
    "import src.load.load_image as li\n",
    "\n",
    "# get list with image paths from folder\n",
    "images = [f\"{img_folder}/{img}\" for img in os.listdir(img_folder)]\n",
    "\n",
    "# only keep images with \"V\" in the name\n",
    "#images = [img for img in images if \"V\" in img]\n",
    "\n",
    "doc = Metashape.Document()\n",
    "\n",
    "chunk = doc.addChunk()\n",
    "\n",
    "# add the images to the chunk\n",
    "chunk.addPhotos(images)\n",
    "\n",
    "# add masks to the images\n",
    "for camera in chunk.cameras:\n",
    "    msk_path = os.path.join(msk_folder, f\"{camera.label}_mask.tif\")\n",
    "    mask = li.load_image(msk_path)\n",
    "    \n",
    "    m_mask = Metashape.Image.fromstring(mask, \n",
    "                                        mask.shape[1], mask.shape[0],\n",
    "                                        channels=' ', datatype='U8')\n",
    "    mask_obj = Metashape.Mask()\n",
    "    mask_obj.setImage(m_mask)\n",
    "    camera.mask = mask_obj\n"
   ],
   "id": "2fd6001973cc318e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddPhotos\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T16:43:10.424368Z",
     "start_time": "2024-08-15T16:43:07.680486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import src.base.find_tie_points as ftp\n",
    "import src.load.load_image as li\n",
    "import src.display.display_images as di\n",
    "import numpy as np\n",
    "tp_dict = {}\n",
    "conf_dict = {}\n",
    "\n",
    "tpd = ftp.TiePointDetector('lightglue', min_conf_value=0.7)\n",
    "\n",
    "# find matches between the images\n",
    "for camera1 in chunk.cameras:\n",
    "    for camera2 in chunk.cameras:\n",
    "        if camera1.label == camera2.label:\n",
    "            continue\n",
    "            \n",
    "        if (camera2.label, camera1.label) in tp_dict:\n",
    "            continue\n",
    "                \n",
    "        print(camera1.label, camera2.label)\n",
    "        \n",
    "        img1 = li.load_image(os.path.join(img_folder, f\"{camera1.label}.tif\"))\n",
    "        img2 = li.load_image(os.path.join(img_folder, f\"{camera2.label}.tif\"))\n",
    "        if len(img1.shape) == 3:\n",
    "            img1 = img1[0,:,:]\n",
    "        if len(img2.shape) == 3:\n",
    "            img2 = img2[0,:,:]\n",
    "        \n",
    "        mask1 = li.load_image(os.path.join(msk_folder, f\"{camera1.label}_mask.tif\"))\n",
    "        mask2 = li.load_image(os.path.join(msk_folder, f\"{camera2.label}_mask.tif\"))\n",
    "        \n",
    "        \n",
    "        # find tie points\n",
    "        tps, conf = tpd.find_tie_points(img1, img2, mask1=mask1, mask2=mask2)\n",
    "        \n",
    "        # store tie points and confidence\n",
    "        tp_dict[(camera1.label, camera2.label)] = tps\n",
    "        conf_dict[(camera1.label, camera2.label)] = conf\n",
    "        print(tps.shape)\n",
    "        \n"
   ],
   "id": "d7efb30fc90ea0a7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m tpd \u001B[38;5;241m=\u001B[39m ftp\u001B[38;5;241m.\u001B[39mTiePointDetector(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlightglue\u001B[39m\u001B[38;5;124m'\u001B[39m, min_conf_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# find matches between the images\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m camera1 \u001B[38;5;129;01min\u001B[39;00m chunk\u001B[38;5;241m.\u001B[39mcameras:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m camera2 \u001B[38;5;129;01min\u001B[39;00m chunk\u001B[38;5;241m.\u001B[39mcameras:\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m camera1\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m==\u001B[39m camera2\u001B[38;5;241m.\u001B[39mlabel:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'chunk' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_camera_by_label(chunk, label):\n",
    "    for camera in chunk.cameras:\n",
    "        if camera.label == label:\n",
    "            return camera\n",
    "    return None\n"
   ],
   "id": "e4812064f7ea6402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T08:47:23.070125Z",
     "start_time": "2024-08-15T08:47:22.633610Z"
    }
   },
   "cell_type": "code",
   "source": "doc.save(\"/home/fdahle/Desktop/agi_test/agi_test.psx\")",
   "id": "ed867f36889cf799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SaveProject: path = /home/fdahle/Desktop/agi_test/agi_test.psx\n",
      "saved project in 0.01331 sec\n",
      "LoadProject: path = /home/fdahle/Desktop/agi_test/agi_test.psx\n",
      "loaded project in 0.001192 sec\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:26:56.862266Z",
     "start_time": "2024-08-15T15:26:56.405087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from plyfile import PlyData\n",
    "\n",
    "fld = \"/home/fdahle/Desktop/agi_test/point_cloud_small\"\n",
    "\n",
    "# iterate all ply files in a fld\n",
    "for file in os.listdir(fld):\n",
    "    \n",
    "    if not file.endswith(\".ply\"):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(fld, file)\n",
    "    output_path = file_path.replace(\".ply\", \".csv\")\n",
    "    \n",
    "    ply_data = PlyData.read(file_path)\n",
    "    vertex_data = ply_data['vertex'].data\n",
    "    column_names = vertex_data.dtype.names  # Get the names of all columns\n",
    "    vertex_arr= np.vstack([vertex_data[name] for name in column_names]).T\n",
    "    \n",
    "    print(file, vertex_arr.shape)\n",
    "    np.savetxt(output_path, vertex_arr, delimiter=\";\")\n"
   ],
   "id": "95940c1fc18ef195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0.ply (3388, 4)\n",
      "p1.ply (10204, 4)\n",
      "p2.ply (7558, 4)\n",
      "points2.ply (7148, 4)\n",
      "tracks.ply (10575, 1)\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:24:04.410225Z",
     "start_time": "2024-08-15T12:24:04.114141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "points_per_image = {}\n",
    "\n",
    "# iterate over all tie points\n",
    "for key, value in tp_dict.items():\n",
    "    \n",
    "    # get the two ids\n",
    "    img1_id = key[0]\n",
    "    img2_id = key[1]\n",
    "    \n",
    "    # create an empty numpy array for each image if it does not exist\n",
    "    if img1_id not in points_per_image:\n",
    "        points_per_image[img1_id] = np.zeros((0, 2))\n",
    "    if img2_id not in points_per_image:\n",
    "        points_per_image[img2_id] = np.zeros((0, 2))\n",
    "    \n",
    "    # get the points per image\n",
    "    points1 = value[:, :2]\n",
    "    points2 = value[:, 2:]\n",
    "\n",
    "    # add the points to the numpy array\n",
    "    points_per_image[img1_id] = np.vstack((points_per_image[img1_id], points1))\n",
    "    points_per_image[img2_id] = np.vstack((points_per_image[img2_id], points2))\n",
    "    \n",
    "import pandas as pd\n",
    "import src.export.export_ply as ep\n",
    "counter = 0\n",
    "for key, val in points_per_image.items():\n",
    "    \n",
    "    # remove duplicates\n",
    "    val = np.unique(val, axis=0)\n",
    "    \n",
    "    # convert numpy to pandas\n",
    "    df = pd.DataFrame(val, columns=[\"x\", \"y\"])\n",
    "    \n",
    "    # add column size with default value of 10\n",
    "    df[\"size\"] = 10.0\n",
    "    \n",
    "    # add column id with a range of 0 to the number of points\n",
    "    df[\"id\"] = range(len(df))\n",
    "    \n",
    "    # export as ply\n",
    "    ply_path = f\"/home/fdahle/Desktop/agi_test/custom/p{counter}.ply\"\n",
    "    ep.export_ply(df, ply_path)\n",
    "    \n",
    "    print(df.shape)\n",
    "\n",
    "    counter = counter + 1\n",
    "    "
   ],
   "id": "6681e101b9c2e30d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m points_per_image \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# iterate over all tie points\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m tp_dict\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      5\u001B[0m     \n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# get the two ids\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     img1_id \u001B[38;5;241m=\u001B[39m key[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      8\u001B[0m     img2_id \u001B[38;5;241m=\u001B[39m key[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tp_dict' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:12:56.019761Z",
     "start_time": "2024-08-15T15:12:55.881009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "folder = \"/home/fdahle/Desktop/agi_test/point_cloud_new\"\n",
    "\n",
    "xml_path = os.path.join(folder, \"doc.xml\")\n",
    "# Load the XML file\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "def save_ply_file(file_path, points):\n",
    "    \"\"\"\n",
    "    Saves a PLY file with given points.\n",
    "    :param file_name: Name of the output PLY file.\n",
    "    :param points: List of points where each point is (x, y, z, id).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # PLY header\n",
    "        header = (\n",
    "            \"ply\\n\"\n",
    "            \"format binary_little_endian 1.0\\n\"\n",
    "            f\"element vertex {len(points)}\\n\"\n",
    "            \"property float x\\n\"\n",
    "            \"property float y\\n\"\n",
    "            \"property float z\\n\"\n",
    "            \"property int id\\n\"\n",
    "            \"end_header\\n\"\n",
    "        )\n",
    "        file.write(header.encode('ascii'))\n",
    "        \n",
    "        # Write point data\n",
    "        for point in points:\n",
    "            file.write(struct.pack('<fffI', point[0], point[1], point[2], point[3]))\n",
    "    \n",
    "    # get file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Find the correct element in the XML and update the count attribute\n",
    "    for elem in root.findall('projections'):\n",
    "        if elem.attrib['path'] == file_name:\n",
    "            elem.set('count', str(len(points)))\n",
    "\n",
    "\n",
    "def generate_ply_files(tie_points_dict, fld):\n",
    "    \"\"\"\n",
    "    Generates PLY files from the tie points dictionary.\n",
    "    :param tie_points_dict: Dictionary where the key is (image1, image2) and the value is a numpy array with shape (x,4).\n",
    "    \"\"\"\n",
    "    image_names = sorted(set([key[0] for key in tie_points_dict.keys()] + [key[1] for key in tie_points_dict.keys()]))\n",
    "    image_to_index = {name: idx for idx, name in enumerate(image_names)}\n",
    "\n",
    "    # Initialize a dictionary to hold the points for each image\n",
    "    image_points = {image: [] for image in image_names}\n",
    "    point_id = 0\n",
    "\n",
    "    # Iterate over the tie points\n",
    "    for (image1, image2), points in tie_points_dict.items():\n",
    "        for point in points:\n",
    "            x1, y1, x2, y2 = point\n",
    "            \n",
    "            # Add point to image1's PLY data\n",
    "            image_points[image1].append((x1, y1, 10.0, point_id))\n",
    "            \n",
    "            # Add point to image2's PLY data\n",
    "            image_points[image2].append((x2, y2, 10.0, point_id))\n",
    "            \n",
    "            # Increment the ID for the next point\n",
    "            point_id += 1\n",
    "\n",
    "    # Save PLY files for each image\n",
    "    for image, points in image_points.items():\n",
    "        index = image_to_index[image]\n",
    "        file_name = f\"p{index}.ply\"\n",
    "        file_path = os.path.join(fld, file_name)\n",
    "        save_ply_file(file_path, points)\n",
    "\n",
    "\n",
    "def save_tracks_ply_file(file_path, num_matches):\n",
    "    \"\"\"\n",
    "    Saves a PLY file with the lengths of all matches.\n",
    "    :param file_path: Path of the output PLY file.\n",
    "    :param matches: List of match lengths.\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        # PLY header\n",
    "        header = (\n",
    "            \"ply\\n\"\n",
    "            \"format binary_little_endian 1.0\\n\"\n",
    "            f\"element vertex {num_matches}\\n\"\n",
    "            \"property uchar color\\n\"\n",
    "            \"end_header\\n\"\n",
    "        )\n",
    "        file.write(header.encode('ascii'))\n",
    "        \n",
    "        # Write match lengths with ID 1\n",
    "        for match_length in range(num_matches):\n",
    "            file.write(struct.pack('<B', 1))  # Using 1 as the color/ID (uchar)\n",
    "\n",
    "\n",
    "# get number of matches\n",
    "num_matches = 0\n",
    "for key, val in tp_dict.items():\n",
    "    num_matches += val.shape[0]\n",
    "\n",
    "print(num_matches)\n",
    "\n",
    "generate_ply_files(tp_dict, folder)\n",
    "save_tracks_ply_file(os.path.join(folder, \"tracks.ply\"), num_matches)\n",
    "# Save the updated XML back to file\n",
    "tree.write(xml_path)\n"
   ],
   "id": "5d2efa103467d29e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43421\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89a9263d15bf6a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "xml_path = os.path.join(folder, \"doc.xml\")\n",
    "\n",
    "# Load the XML file\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define the file paths\n",
    "csv_files = {\n",
    "    \"p0\": '/mnt/data/p0.csv',\n",
    "    \"p1\": '/mnt/data/p1.csv',\n",
    "    \"p2\": '/mnt/data/p2.csv',\n",
    "    \"p3\": '/mnt/data/p3.csv',\n",
    "    \"p4\": '/mnt/data/p4.csv',\n",
    "    \"p5\": '/mnt/data/p5.csv'\n",
    "}\n",
    "\n",
    "# Update the counts in the XML\n",
    "for camera_id, file_path in csv_files.items():\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    # Count the number of unique IDs\n",
    "    unique_ids = data.iloc[:, -1].nunique()\n",
    "    \n",
    "    # Find the correct element in the XML and update the count attribute\n",
    "    for elem in root.findall('projections'):\n",
    "        if elem.attrib['camera_id'] == camera_id[-1]:  # camera_id corresponds to last character of key\n",
    "            elem.set('count', str(unique_ids))\n",
    "\n",
    "# Also update the tracks count with the number of unique IDs in tracks.csv\n",
    "tracks_data = pd.read_csv('/mnt/data/tracks.csv', delimiter=';')\n",
    "unique_tracks = tracks_data.iloc[:, 0].nunique()\n",
    "\n",
    "for elem in root.findall('tracks'):\n",
    "    elem.set('count', str(unique_tracks))\n",
    "\n",
    "# Save the updated XML back to file\n",
    "tree.write('/mnt/data/updated_doc.xml')\n"
   ],
   "id": "2476aaed4e944103"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:01:11.055894Z",
     "start_time": "2024-08-15T15:01:11.028213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load tracks.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "folder = \"/home/fdahle/Desktop/agi_test/point_cloud\"\n",
    "tracks = pd.read_csv(os.path.join(folder, \"tracks.csv\"), delimiter=\";\", header=None, names=[\"point_id\"])\n",
    "\n",
    "# count unique values\n",
    "unique_values = tracks[\"point_id\"].value_counts()\n",
    "print(unique_values)\n",
    "print(len(unique_values))"
   ],
   "id": "290578c1a1585dc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_id\n",
      "172.0    69\n",
      "175.0    68\n",
      "178.0    65\n",
      "183.0    63\n",
      "174.0    63\n",
      "         ..\n",
      "40.0      1\n",
      "39.0      1\n",
      "32.0      1\n",
      "34.0      1\n",
      "208.0     1\n",
      "Name: count, Length: 172, dtype: int64\n",
      "172\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:13:05.079443Z",
     "start_time": "2024-08-15T15:13:05.054905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def create_zip_from_folder(folder_path, output_zip_path):\n",
    "    \"\"\"\n",
    "    Creates a zip file containing all .ply and .xml files in a specified folder.\n",
    "    :param folder_path: Path to the folder containing the files.\n",
    "    :param output_zip_path: Path where the output zip file will be saved.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_STORED) as zipf:\n",
    "        for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.ply') or filename.endswith('.xml'):\n",
    "                    file_path = os.path.join(foldername, filename)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/home/fdahle/Desktop/agi_test/point_cloud_new\"\n",
    "output_zip_path = \"/home/fdahle/Desktop/agi_test/point_cloud_new.zip\"\n",
    "create_zip_from_folder(folder_path, output_zip_path)\n"
   ],
   "id": "45ddd0c24ef460bf",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T15:25:11.834503Z",
     "start_time": "2024-08-15T15:25:08.415712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import Metashape\n",
    "doc = Metashape.Document()\n",
    "doc.open(\"/home/fdahle/Desktop/agi_test/agi_small.psx\")\n",
    "chunk = doc.chunk\n",
    "chunk.alignCameras(adaptive_fitting=True, reset_alignment=True)\n",
    "doc.save()"
   ],
   "id": "2ede215790ea7434",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoadProject: path = /home/fdahle/Desktop/agi_test/agi_small.psx\n",
      "loaded project in 0.000939 sec\n",
      "AlignCameras: adaptive_fitting = on, reset_alignment = on\n",
      "processing matches... done in 0.010892 sec\n",
      "selecting camera groups... \n",
      "groups: 3\n",
      "n groups: 1, total: 3, minmax: [3, 3]\n",
      "done in 9.2e-05 sec\n",
      "scheduled 1 alignment groups\n",
      "saved camera partition in 0.001295 sec\n",
      "loaded camera partition in 2.2e-05 sec\n",
      "processing block: 3 photos\n",
      "pair 0 and 2: 368 robust from 371\n",
      "pair 0 and 1: 3007 robust from 3017\n",
      "pair 1 and 2: 7187 robust from 7187\n",
      "evaluating initial pair...\n",
      "initial pair evaluated in 0.189142 sec.\n",
      "initial pair unstable, considering additional pairs...\n",
      "additional pairs considered in 3e-06 sec.\n",
      "optimizing initial pair...\n",
      "****************************************************************************************************\n",
      "default pair score: n_aligned: 2, n_points_tier: 0, accuracy_tier: 0, reprojection_error: 0, accuracy: 0, n_points: 0, ids: [1, 2]\n",
      "initial pair optimized in 1.34593 sec.\n",
      "pair 1 and 2: 7187 robust\n",
      "adding 7187 points, 0 far, 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxx 2.55102 -> 2.5356\n",
      "adding 0 points, 0 far, 0 inaccurate, 0 invisible, 0 weak\n",
      "optimized in 0.060352 seconds\n",
      "(3 px, 2 3d) sigma filtering...\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.5356 -> 2.51708\n",
      "point variance: 1.77916 px, threshold: 5.33749 px\n",
      "adding 0 points, 42 far (5.33749 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxx"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optimal pair not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxx 2.43802 -> 2.42296\n",
      "point variance: 1.71263 px, threshold: 5.13789 px\n",
      "adding 4 points, 7 far (5.13789 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.41961 -> 2.4068\n",
      "point variance: 1.7012 px, threshold: 5.10361 px\n",
      "adding 7 points, 6 far (5.10361 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.40802 -> 2.39551\n",
      "point variance: 1.69322 px, threshold: 5.07967 px\n",
      "adding 10 points, 3 far (5.07967 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.40428 -> 2.39095\n",
      "point variance: 1.69 px, threshold: 5.06999 px\n",
      "adding 7 points, 1 far (5.06999 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "optimized in 0.495101 seconds\n",
      "f 13195.3, cx 0, cy 0, k1 -0.027006, k2 0, k3 0\n",
      "finished sfm in 2.34881 seconds\n",
      "loaded projections in 1.4e-05 sec\n",
      "tracks initialized in 0.000803 sec\n",
      "adding 7187 points, 0 far, 0 inaccurate, 0 invisible, 0 weak\n",
      "loaded camera partition in 3e-05 sec\n",
      "1 blocks: 3\n",
      "calculating match counts... done in 5e-06 sec\n",
      "overlapping groups selected in 2e-06 sec\n",
      "loaded camera partition in 1.6e-05 sec\n",
      "1 blocks: 3\n",
      "block: 1 sensors, 3 cameras, 10575 points, 21150 projections\n",
      "block_sensors: 0.000839233 MB (0.000839233 MB allocated)\n",
      "block_cameras: 0.0011673 MB (0.0011673 MB allocated)\n",
      "block_points: 0.484085 MB (0.65799 MB allocated)\n",
      "block_tracks: 0.0403404 MB (0.0403404 MB allocated)\n",
      "block_obs: 0.96817 MB (0.96817 MB allocated)\n",
      "block_ofs: 0.0806885 MB (0.0806885 MB allocated)\n",
      "block_fre: 0 MB (0 MB allocated)\n",
      "final block size: 3\n",
      "adding 7187 points, 0 far, 0 inaccurate, 0 invisible, 0 weak\n",
      "(3 px, 2 3d) sigma filtering...\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.29921 -> 2.27329\n",
      "point variance: 1.60683 px, threshold: 4.8205 px\n",
      "adding 0 points, 49 far (4.8205 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.18444 -> 2.1763\n",
      "point variance: 1.53827 px, threshold: 4.61482 px\n",
      "adding 7 points, 12 far (4.61482 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.17091 -> 2.1604\n",
      "point variance: 1.52704 px, threshold: 4.58111 px\n",
      "adding 16 points, 7 far (4.58111 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.17047 -> 2.15265\n",
      "point variance: 1.52156 px, threshold: 4.56468 px\n",
      "adding 8 points, 3 far (4.56468 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "adjusting: xxxxxxxxxxxxxxxxxxxx 2.15795 -> 2.1314\n",
      "point variance: 1.50654 px, threshold: 4.51961 px\n",
      "adding 5 points, 4 far (4.51961 px threshold), 0 inaccurate, 0 invisible, 0 weak\n",
      "optimized in 0.481611 seconds\n",
      "coordinates applied in 0 sec\n",
      "SaveProject: path = /home/fdahle/Desktop/agi_test/agi_small.psx\n",
      "saved project in 0.022477 sec\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b40b883e4aad61ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
