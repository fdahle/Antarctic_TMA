{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:55:30.034933Z",
     "start_time": "2024-08-19T11:55:30.022177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "8f6a57d3bb739851",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:55:30.362208Z",
     "start_time": "2024-08-19T11:55:30.345775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_folder = \"/home/fdahle/Desktop/agi_test/images\"\n",
    "msk_folder = \"/home/fdahle/Desktop/agi_test/masks\""
   ],
   "id": "8d93631926370490",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:55:31.054507Z",
     "start_time": "2024-08-19T11:55:31.034195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import src.base.create_mask as cm\n",
    "import src.load.load_image as li\n",
    "import src.export.export_tiff as et\n",
    "\n",
    "import os\n",
    "for img in os.listdir(img_folder):\n",
    "    \n",
    "    img_path = os.path.join(img_folder, img)\n",
    "    img_id = img.split(\".\")[0]\n",
    "    print(img_path)\n",
    "    \n",
    "    # load image\n",
    "    img = li.load_image(img_path)\n",
    "    mask = cm.create_mask(img, use_database=True, image_id=img_id)\n",
    "    et.export_tiff(mask, os.path.join(msk_folder, f\"{img_id}_mask.tif\"), overwrite=True)\n",
    "\"\"\""
   ],
   "id": "dce4fc6db4fcb357",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport src.base.create_mask as cm\\nimport src.load.load_image as li\\nimport src.export.export_tiff as et\\n\\nimport os\\nfor img in os.listdir(img_folder):\\n    \\n    img_path = os.path.join(img_folder, img)\\n    img_id = img.split(\".\")[0]\\n    print(img_path)\\n    \\n    # load image\\n    img = li.load_image(img_path)\\n    mask = cm.create_mask(img, use_database=True, image_id=img_id)\\n    et.export_tiff(mask, os.path.join(msk_folder, f\"{img_id}_mask.tif\"), overwrite=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:55:31.884616Z",
     "start_time": "2024-08-19T11:55:31.866784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import src.base.rotate_image as ri\n",
    "import src.load.load_image as li\n",
    "import src.export.export_tiff as et\n",
    "import os\n",
    "\n",
    "# rotate vertical images \n",
    "for img in os.listdir(img_folder):\n",
    "    if \"V\" in img:\n",
    "        print(img)\n",
    "        img_path = os.path.join(img_folder, img)\n",
    "        img_id = img.split(\".\")[0]\n",
    "        img = li.load_image(img_path)\n",
    "        img = ri.rotate_image(img, 180)\n",
    "        et.export_tiff(img, img_path, overwrite=True)\n",
    "\n",
    "for img in os.listdir(msk_folder):\n",
    "    if \"V\" in img:\n",
    "        print(img)\n",
    "        img_path = os.path.join(msk_folder, img)\n",
    "        img_id = img.split(\".\")[0]\n",
    "        img = li.load_image(img_path)\n",
    "        img = ri.rotate_image(img, 180)\n",
    "        et.export_tiff(img, img_path, overwrite=True)\n",
    "\"\"\""
   ],
   "id": "ca125d76807653b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport src.base.rotate_image as ri\\nimport src.load.load_image as li\\nimport src.export.export_tiff as et\\nimport os\\n\\n# rotate vertical images \\nfor img in os.listdir(img_folder):\\n    if \"V\" in img:\\n        print(img)\\n        img_path = os.path.join(img_folder, img)\\n        img_id = img.split(\".\")[0]\\n        img = li.load_image(img_path)\\n        img = ri.rotate_image(img, 180)\\n        et.export_tiff(img, img_path, overwrite=True)\\n\\nfor img in os.listdir(msk_folder):\\n    if \"V\" in img:\\n        print(img)\\n        img_path = os.path.join(msk_folder, img)\\n        img_id = img.split(\".\")[0]\\n        img = li.load_image(img_path)\\n        img = ri.rotate_image(img, 180)\\n        et.export_tiff(img, img_path, overwrite=True)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:55:41.011341Z",
     "start_time": "2024-08-19T11:55:33.910550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import Metashape\n",
    "import src.load.load_image as li\n",
    "\n",
    "# get list with image paths from folder\n",
    "images = [f\"{img_folder}/{img}\" for img in os.listdir(img_folder)]\n",
    "\n",
    "# only keep images with \"V\" in the name\n",
    "#images = [img for img in images if \"V\" in img]\n",
    "\n",
    "doc = Metashape.Document()\n",
    "\n",
    "chunk = doc.addChunk()\n",
    "\n",
    "# add the images to the chunk\n",
    "chunk.addPhotos(images)\n",
    "\n",
    "# add masks to the images\n",
    "for camera in chunk.cameras:\n",
    "    msk_path = os.path.join(msk_folder, f\"{camera.label}_mask.tif\")\n",
    "    mask = li.load_image(msk_path)\n",
    "    \n",
    "    m_mask = Metashape.Image.fromstring(mask, \n",
    "                                        mask.shape[1], mask.shape[0],\n",
    "                                        channels=' ', datatype='U8')\n",
    "    mask_obj = Metashape.Mask()\n",
    "    mask_obj.setImage(m_mask)\n",
    "    camera.mask = mask_obj\n"
   ],
   "id": "2fd6001973cc318e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddPhotos\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:57:17.663746Z",
     "start_time": "2024-08-19T11:55:57.757969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import src.base.find_tie_points as ftp\n",
    "import src.load.load_image as li\n",
    "import src.display.display_images as di\n",
    "import numpy as np\n",
    "tp_dict = {}\n",
    "conf_dict = {}\n",
    "\n",
    "tpd = ftp.TiePointDetector('lightglue', min_conf_value=0.7)\n",
    "\n",
    "# find matches between the images\n",
    "for camera1 in chunk.cameras:\n",
    "    for camera2 in chunk.cameras:\n",
    "        if camera1.label == camera2.label:\n",
    "            continue\n",
    "            \n",
    "        if (camera2.label, camera1.label) in tp_dict:\n",
    "            continue\n",
    "                \n",
    "        print(camera1.label, camera2.label)\n",
    "        \n",
    "        img1 = li.load_image(os.path.join(img_folder, f\"{camera1.label}.tif\"))\n",
    "        img2 = li.load_image(os.path.join(img_folder, f\"{camera2.label}.tif\"))\n",
    "        if len(img1.shape) == 3:\n",
    "            img1 = img1[0,:,:]\n",
    "        if len(img2.shape) == 3:\n",
    "            img2 = img2[0,:,:]\n",
    "        \n",
    "        mask1 = li.load_image(os.path.join(msk_folder, f\"{camera1.label}_mask.tif\"))\n",
    "        mask2 = li.load_image(os.path.join(msk_folder, f\"{camera2.label}_mask.tif\"))\n",
    "        \n",
    "        \n",
    "        # find tie points\n",
    "        tps, conf = tpd.find_tie_points(img1, img2, mask1=mask1, mask2=mask2)\n",
    "        \n",
    "        # store tie points and confidence\n",
    "        tp_dict[(camera1.label, camera2.label)] = tps\n",
    "        conf_dict[(camera1.label, camera2.label)] = conf\n",
    "        print(tps.shape)\n",
    "        \n"
   ],
   "id": "d7efb30fc90ea0a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA215232V0360 CA215232V0361\n",
      "(3017, 4)\n",
      "CA215232V0360 CA215232V0362\n",
      "(371, 4)\n",
      "CA215232V0360 CA215331L0391\n",
      "(0, 4)\n",
      "CA215232V0360 CA215331L0392\n",
      "(0, 4)\n",
      "CA215232V0360 CA215331L0393\n",
      "(0, 4)\n",
      "CA215232V0361 CA215232V0362\n",
      "(7187, 4)\n",
      "CA215232V0361 CA215331L0391\n",
      "(4, 4)\n",
      "CA215232V0361 CA215331L0392\n",
      "(141, 4)\n",
      "CA215232V0361 CA215331L0393\n",
      "(7, 4)\n",
      "CA215232V0362 CA215331L0391\n",
      "(2, 4)\n",
      "CA215232V0362 CA215331L0392\n",
      "(44, 4)\n",
      "CA215232V0362 CA215331L0393\n",
      "(382, 4)\n",
      "CA215331L0391 CA215331L0392\n",
      "(14011, 4)\n",
      "CA215331L0391 CA215331L0393\n",
      "(7220, 4)\n",
      "CA215331L0392 CA215331L0393\n",
      "(11035, 4)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:10:08.987083Z",
     "start_time": "2024-08-19T12:05:47.066785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import src.display.display_images as di\n",
    "for key, val in tp_dict.items():\n",
    "    \n",
    "    if val.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    style_config = {\n",
    "        'title': f\"{key[0]} - {key[1]}: {val.shape[0]}\",\n",
    "    }\n",
    "    img1 = li.load_image(key[0])\n",
    "    img2 = li.load_image(key[1])\n",
    "    \n",
    "    di.display_images([img1, img2], tie_points=val, style_config=style_config)\n",
    "    "
   ],
   "id": "70ea2b29d765ee79",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_camera_by_label(chunk, label):\n",
    "    for camera in chunk.cameras:\n",
    "        if camera.label == label:\n",
    "            return camera\n",
    "    return None\n"
   ],
   "id": "e4812064f7ea6402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doc.save(\"/home/fdahle/Desktop/agi_test/agi_test.psx\")",
   "id": "ed867f36889cf799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from plyfile import PlyData\n",
    "\n",
    "fld = \"/home/fdahle/Desktop/agi_test/point_cloud_small\"\n",
    "\n",
    "# iterate all ply files in a fld\n",
    "for file in os.listdir(fld):\n",
    "    \n",
    "    if not file.endswith(\".ply\"):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(fld, file)\n",
    "    output_path = file_path.replace(\".ply\", \".csv\")\n",
    "    \n",
    "    ply_data = PlyData.read(file_path)\n",
    "    vertex_data = ply_data['vertex'].data\n",
    "    column_names = vertex_data.dtype.names  # Get the names of all columns\n",
    "    vertex_arr= np.vstack([vertex_data[name] for name in column_names]).T\n",
    "    \n",
    "    print(file, vertex_arr.shape)\n",
    "    np.savetxt(output_path, vertex_arr, delimiter=\";\")\n"
   ],
   "id": "95940c1fc18ef195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:14:47.017909Z",
     "start_time": "2024-08-19T12:14:43.788539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "points_per_image = {}\n",
    "\n",
    "print(len(tp_dict))\n",
    "\n",
    "# iterate over all tie points\n",
    "for key, value in tp_dict.items():\n",
    "    \n",
    "    # get the two ids\n",
    "    img1_id = key[0]\n",
    "    img2_id = key[1]\n",
    "    \n",
    "    if value.shape[0] < 25:\n",
    "        continue\n",
    "    \n",
    "    # create an empty numpy array for each image if it does not exist\n",
    "    if img1_id not in points_per_image:\n",
    "        points_per_image[img1_id] = np.zeros((0, 2))\n",
    "    if img2_id not in points_per_image:\n",
    "        points_per_image[img2_id] = np.zeros((0, 2))\n",
    "    \n",
    "    # get the points per image\n",
    "    points1 = value[:, :2]\n",
    "    points2 = value[:, 2:]\n",
    "\n",
    "    # add the points to the numpy array\n",
    "    points_per_image[img1_id] = np.vstack((points_per_image[img1_id], points1))\n",
    "    points_per_image[img2_id] = np.vstack((points_per_image[img2_id], points2))\n",
    "    \n",
    "import pandas as pd\n",
    "import src.export.export_ply as ep\n",
    "counter = 0\n",
    "for key, val in points_per_image.items():\n",
    "            \n",
    "    # remove duplicates\n",
    "    val = np.unique(val, axis=0)\n",
    "    \n",
    "    # skip tie-points with less than 25 points\n",
    "    if val.shape[0] < 25:\n",
    "        continue\n",
    "    \n",
    "    # convert numpy to pandas\n",
    "    df = pd.DataFrame(val, columns=[\"x\", \"y\"])\n",
    "    \n",
    "    # add column size with default value of 10\n",
    "    df[\"size\"] = 10.0\n",
    "    \n",
    "    # add column id with a range of 0 to the number of points\n",
    "    df[\"id\"] = range(len(df))\n",
    "    \n",
    "    # export as ply\n",
    "    ply_path = f\"/home/fdahle/Desktop/agi_test/custom_vertical/p{counter}.ply\"\n",
    "    ep.export_ply(df, ply_path)\n",
    "    \n",
    "    print(df.shape)\n",
    "\n",
    "    counter = counter + 1\n",
    "    "
   ],
   "id": "6681e101b9c2e30d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(3068, 4)\n",
      "(10333, 4)\n",
      "(7973, 4)\n",
      "(24841, 4)\n",
      "(18507, 4)\n",
      "(15553, 4)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:20:30.077677Z",
     "start_time": "2024-08-19T12:20:29.999996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "folder = \"/home/fdahle/Desktop/agi_test/point_cloud_vertical\"\n",
    "\n",
    "xml_path = os.path.join(folder, \"doc.xml\")\n",
    "# Load the XML file\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "def save_ply_file(file_path, points):\n",
    "    \"\"\"\n",
    "    Saves a PLY file with given points.\n",
    "    :param file_name: Name of the output PLY file.\n",
    "    :param points: List of points where each point is (x, y, z, id).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # PLY header\n",
    "        header = (\n",
    "            \"ply\\n\"\n",
    "            \"format binary_little_endian 1.0\\n\"\n",
    "            f\"element vertex {len(points)}\\n\"\n",
    "            \"property float x\\n\"\n",
    "            \"property float y\\n\"\n",
    "            \"property float z\\n\"\n",
    "            \"property int id\\n\"\n",
    "            \"end_header\\n\"\n",
    "        )\n",
    "        file.write(header.encode('ascii'))\n",
    "        \n",
    "        # Write point data\n",
    "        for point in points:\n",
    "            file.write(struct.pack('<fffI', point[0], point[1], point[2], point[3]))\n",
    "    \n",
    "    # get file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Find the correct element in the XML and update the count attribute\n",
    "    for elem in root.findall('projections'):\n",
    "        if elem.attrib['path'] == file_name:\n",
    "            elem.set('count', str(len(points)))\n",
    "\n",
    "\n",
    "def generate_ply_files(tie_points_dict, fld):\n",
    "    \"\"\"\n",
    "    Generates PLY files from the tie points dictionary.\n",
    "    :param tie_points_dict: Dictionary where the key is (image1, image2) and the value is a numpy array with shape (x,4).\n",
    "    \"\"\"\n",
    "    image_names = sorted(set([key[0] for key in tie_points_dict.keys()] + [key[1] for key in tie_points_dict.keys()]))\n",
    "    image_to_index = {name: idx for idx, name in enumerate(image_names)}\n",
    "\n",
    "    # Initialize a dictionary to hold the points for each image\n",
    "    image_points = {image: [] for image in image_names}\n",
    "    point_id = 0\n",
    "\n",
    "    # Iterate over the tie points\n",
    "    for (image1, image2), points in tie_points_dict.items():\n",
    "        for point in points:\n",
    "            x1, y1, x2, y2 = point\n",
    "            \n",
    "            # Add point to image1's PLY data\n",
    "            image_points[image1].append((x1, y1, 10.0, point_id))\n",
    "            \n",
    "            # Add point to image2's PLY data\n",
    "            image_points[image2].append((x2, y2, 10.0, point_id))\n",
    "            \n",
    "            # Increment the ID for the next point\n",
    "            point_id += 1\n",
    "\n",
    "    # Save PLY files for each image\n",
    "    for image, points in image_points.items():\n",
    "        index = image_to_index[image]\n",
    "        file_name = f\"p{index}.ply\"\n",
    "        file_path = os.path.join(fld, file_name)\n",
    "        save_ply_file(file_path, points)\n",
    "\n",
    "\n",
    "def save_tracks_ply_file(file_path, num_matches):\n",
    "    \"\"\"\n",
    "    Saves a PLY file with the lengths of all matches.\n",
    "    :param file_path: Path of the output PLY file.\n",
    "    :param matches: List of match lengths.\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        # PLY header\n",
    "        header = (\n",
    "            \"ply\\n\"\n",
    "            \"format binary_little_endian 1.0\\n\"\n",
    "            f\"element vertex {num_matches}\\n\"\n",
    "            \"property uchar color\\n\"\n",
    "            \"end_header\\n\"\n",
    "        )\n",
    "        file.write(header.encode('ascii'))\n",
    "        \n",
    "        # Write match lengths with ID 1\n",
    "        for match_length in range(num_matches):\n",
    "            file.write(struct.pack('<B', 1))  # Using 1 as the color/ID (uchar)\n",
    "\n",
    "\n",
    "# remove all non vertical images\n",
    "tp_vertical = {}\n",
    "for key, val in tp_dict.copy().items():\n",
    "    if \"V\" in key[0] and \"V\" in key[1]:\n",
    "        tp_vertical[key] = val\n",
    "\n",
    "# get number of matches\n",
    "num_matches = 0\n",
    "for key, val in tp_vertical.items():\n",
    "    num_matches += val.shape[0]\n",
    "\n",
    "print(num_matches)\n",
    "\n",
    "generate_ply_files(tp_vertical, folder)\n",
    "save_tracks_ply_file(os.path.join(folder, \"tracks.ply\"), num_matches)\n",
    "# Save the updated XML back to file\n",
    "tree.write(xml_path)\n"
   ],
   "id": "5d2efa103467d29e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10575\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89a9263d15bf6a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "xml_path = os.path.join(folder, \"doc.xml\")\n",
    "\n",
    "# Load the XML file\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define the file paths\n",
    "csv_files = {\n",
    "    \"p0\": '/mnt/data/p0.csv',\n",
    "    \"p1\": '/mnt/data/p1.csv',\n",
    "    \"p2\": '/mnt/data/p2.csv',\n",
    "}\n",
    "\n",
    "# Update the counts in the XML\n",
    "for camera_id, file_path in csv_files.items():\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "    # Count the number of unique IDs\n",
    "    unique_ids = data.iloc[:, -1].nunique()\n",
    "    \n",
    "    # Find the correct element in the XML and update the count attribute\n",
    "    for elem in root.findall('projections'):\n",
    "        if elem.attrib['camera_id'] == camera_id[-1]:  # camera_id corresponds to last character of key\n",
    "            elem.set('count', str(unique_ids))\n",
    "\n",
    "# Also update the tracks count with the number of unique IDs in tracks.csv\n",
    "tracks_data = pd.read_csv('/mnt/data/tracks.csv', delimiter=';')\n",
    "unique_tracks = tracks_data.iloc[:, 0].nunique()\n",
    "\n",
    "for elem in root.findall('tracks'):\n",
    "    elem.set('count', str(unique_tracks))\n",
    "\n",
    "# Save the updated XML back to file\n",
    "tree.write('/mnt/data/updated_doc.xml')\n"
   ],
   "id": "2476aaed4e944103",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load tracks.csv\n",
    "import pandas as pd\n",
    "import os\n",
    "folder = \"/home/fdahle/Desktop/agi_test/point_cloud\"\n",
    "tracks = pd.read_csv(os.path.join(folder, \"tracks.csv\"), delimiter=\";\", header=None, names=[\"point_id\"])\n",
    "\n",
    "# count unique values\n",
    "unique_values = tracks[\"point_id\"].value_counts()\n",
    "print(unique_values)\n",
    "print(len(unique_values))"
   ],
   "id": "290578c1a1585dc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def create_zip_from_folder(folder_path, output_zip_path):\n",
    "    \"\"\"\n",
    "    Creates a zip file containing all .ply and .xml files in a specified folder.\n",
    "    :param folder_path: Path to the folder containing the files.\n",
    "    :param output_zip_path: Path where the output zip file will be saved.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_STORED) as zipf:\n",
    "        for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.ply') or filename.endswith('.xml'):\n",
    "                    file_path = os.path.join(foldername, filename)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/home/fdahle/Desktop/agi_test/point_cloud_new\"\n",
    "output_zip_path = \"/home/fdahle/Desktop/agi_test/point_cloud_new.zip\"\n",
    "create_zip_from_folder(folder_path, output_zip_path)\n"
   ],
   "id": "45ddd0c24ef460bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import Metashape\n",
    "doc = Metashape.Document()\n",
    "doc.open(\"/home/fdahle/Desktop/agi_test/agi_small.psx\")\n",
    "chunk = doc.chunk\n",
    "chunk.alignCameras(adaptive_fitting=True, reset_alignment=True)\n",
    "doc.save()"
   ],
   "id": "2ede215790ea7434",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b40b883e4aad61ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
